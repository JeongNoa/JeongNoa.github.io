---
title: "강화학습_1"
categories: "etc"
changefreq : month
permalink: /Reinforcement/1/
---

강화학습.01
------



이 시리즈는 단단한 강화학습 책을 기반으로 공부한 것을 정리한 것이다.



목차

> [1.1 강화학습이란?](#1.1 강화학습이란?)

> [1.2 강화학습 VS 지도학습, 비지도학습](#1.2 강화학습 VS 지도학습, 비지도학습)

> [1.3 강화학습 예제](#1.3 강화학습 예제)

> [1.4 강화학습의 구성요소](1.4 강화학습의 구성요소)

> [1.5 가치함수를 통한 강화학습](#1.5 가치함수를 통한 강화학습)

> [1.6 요약](#1.6 요약)



### 1.1 강화학습이란?

강화학습이란, 주어진 상황에서 어떤 행동을 취할지 학습하는 것이다.

학습자(agent)는 어떤 행동을 취할지에 대해서 지시를 받는 것이 아니라 오직 시행착오만을 통하여

최대의 보상을 가져다주는 행동을 찾는다.

강화학습에서만의 흥미로운 상황은 특정 행동이 그 직후에 받는 보상뿐 아니라 그 뒤에 이어지는 상황을 

통해 연속적으로 보상에 영향을 받는 것이다. 

이 2가지 특징, 시행착오와 지연된 보상은 강화학습을 다른 방법과 구별되게 만든다.



### 1.2 강화학습 VS 지도학습, 비지도학습

![T]({{ site.url }}{{ site.baseurl }}/assets/images/David_Silver/1/reinforce.jpg){: width="90%" height="90%"}{: .align-center}

잘 알려져 있다 싶이 기계학습 분야는 지도학습, 비지도학습, 강화학습 3가지로 구분된다.

지도학습은 외부에서의 지침이 포함된 학습 데이터로부터 학습하는것, 비지도학습은 지침이 없는

테이터의 집합안에서 숨겨진 구조를 찾는 것이다.



그렇다면 강화학습은 이 둘과 어떤 다른 점이 있기에 구분되는 것일까?

우선 강화학습은 올바른 행동에 대한 지침을 필요로 하지 않는다는 점에서 지도학습과 구분된다.

또한 비지도학습과는 딱히 강화학습은 데이터의 숨겨진 구조를 찾으려고 하지 않는다는 점에서 구분된다. 

숨겨진 구조를 찾는 것은 강화학습에도 도움이 되지만 그것만으론 강화학습의 문제를 풀지 못한다.

따라서 강화학습은 지도학습, 비지도학습과 구분되는 제 3의 기계학습인 것이다.

### 1.3 강화학습 예제



한번 강화학습의 예제를 통해 강화학습의 특징을 살펴보자.

​	 땡칠이가 fps게임을 시작했다. 땡칠이는 게임을 이기려고 상황에 따라 적과 교전하기 위한 계획을       	    	세우고 이를 행동으로 옮기며 실행한다. 그리고 계속된 시행착오를 통해 점점 게임을 잘해나가게 되고 승	리횟수가 올라간다.

위와 같은 강화학습의 상황들은 학습자(땡칠이)와 이를 둘러싼 **주변 환경(게임환경)과의 상호작용**을 

다루고 있다. 주변 환경엔 **불확실한 요소**(적의 위치, 전략)들이 있지만, 학습자는 **목표**(게임의 승리)를 

이루기 위한 방법을 모색한다. 그리고, 행동을 취함(적을 기습)으로서 주변 환경의 미래 환경(교전상황, 

자신과 적의 체력)에  영향을 미치고, 결국 미래의 자신이 취할 수 있는 행동과 기회에 영향을 주게 된다.

그리고 학습자는 자신의 경험을 활용해서 시간이 지나면서 행동의 능력을 키우고, 더욱 적절한 선택을 

해나가게 된다.



### 1.4 강화학습의 구성요소



학습자와 주변환경을 제외하고도 강화학습에는 4가지 중요한 구성요소가 있다.

**정책**, **보상 신호**, **가치 함수**, 그리고 주변환경에 대한 **모델**(필수는 아니다)이다.



**정책**은 특정 시점에서 학습자가 취할 행동을 정한다.

정책 그 자체만으로 행동을 결정가능함으로 강화학습 학습자에 있어 핵심이 되는 부분이다.



**보상 신호**는 강화학습이 성취해야 할 목표를 정의한다.

매 시간마다 부변환경은 학습자에게 **보상**이라 불리는 숫자(보상신호)를 전달하고, 학습자는 장기간에 걸쳐

학습자가 획득하게 되는 보상의 총합을 최대로 만드는 것을 목표로 삼는다. 

따라서 학습자는 보상신호의 크기로부터 자신의 행동의 좋고 나쁨을 판단 가능하게 된다.



보상신호는 무엇이 좋은가를 즉각적으로 알려주지만, **가치 함수**는 장기적인 관점에서 무엇이 좋은가를

알려준다. 특정 상태의 **가치**는 그 상태의 시작점에서 일정 시간동안 학습자가 기대가능한 총량이다. 

따라서 가치는 **장기적 관점**으로 그 상태를 평가한 것이라고 할 수 있다. 따라서 행동의 선택은 가치를 

기준으로 이루어지고(이래야 장기적으로 최대한의 보상을 얻게되기 때문) 가치 추정이

강화학습 알고리즘에서 핵심적 역할을 한다.



마지막으로 환경 **모델**은 환경의 변화를 모사하는데, 이를 통해 환경이 어떻게 변화해 갈지 추정할 수 있게 해준다.

**모델**은 계획을 위해 사용되는데, 이는 미래의 상황을 예측하여 일련의 행동을 결정하는 방법이다.

모델과 계획을 사용하는 강화학습 방법을 **model-based** 방법이라고 한다. 

이와 반대되는 개념으로 **model-free** 방법이 있는데, 이는 시행착오 학습자가 취하는 방법으로 계획과는 

거의 정반대의 방법이다.



### 1.5 가치함수를 통한 강화학습

이번엔 틱택토라는 게임을 생각해 보자. 

![T]({{ site.url }}{{ site.baseurl }}/assets/images/David_Silver/1/tiktak.jpg){: width="90%" height="90%"}{: .align-center}

3행과 3열로 이로어진 격자판으로 두 명이 번갈아 가며 게임을 한다. 한 사람은 X, 다른 사람은 O을 표시하

여 가로, 세로, 대각선 중 하나의 방향으로 연달아 세개의 동일한 표시를 하면 승리하는 게임이다. 이 게임은

숙달되면 무조건 이길 수 있기에, 가끔씩 잘못된 선택을 하는 덜 숙달된 사람과 게임을 하고 있다고 하자.

학습자의 목표는 무조건 승리라 할 때 어떻게 하면 승리할 확률을 최대로 할 수 있을까?



한번 가치 함수를 이용해서 틱택토 문제에 접근해보자. 먼저, 게임에서 나타날 수 있는 모든 상태에 해당 

상태에서 게임 참여자가 승리할 확률의 최신의 추청값을 부여해 표를 만들자. 이 추청값을 상태의 가치로 

생각하면, 전체 표는 학습된 가치 함수가 된다. 상태 A로부터 승리할 확률이 상태 B에서부터보다 높으면 

상태 A가 더 높은 가치를 지니고 있는 것이다. 모든 상태의 초기 가치는 0.5로, 즉 승리 확률을 50%로 

설정한다.



이제 상대방과 여러 번 게임을 진행하며, 지속적으로 자신의 선택에 따른 상태와 숫자표의 가치를 면밀히 

따져본다. 대부분의 시간동안 가치를 최대로 하는 선택, 즉 **탐욕적으로 선**택한다. 하지만 간혹 무작위로 

선택하기도 하는데, 이를 무작위 선택이 아니면 경험하지 못할 곳을 경험하게 해준다는 의미에서 **탐험적**

**선택**이라 한다.



게임을 하는 동안 게임 참여자는 지속적으로 자신이 처한 상태의 가치를 변화시키며 가치가 승리 확률에 

대한 더욱 정확한 추정이 되도록 노력한다. 더 정확히는, 이전 상태의 현재 가치가 나중 상태의 현재 가치에 

가까워지도록 생신되는 것이다. 탐욕스로운 선택 이전의 상태를 St, 선택 이후의 상태를 St+1라고 하면,

V(St)로 표현되는 St추정값의 갱신은 다음과 같다.



> V(St) ←V(St)  + α[V(St+1) - V(St)]



여기서 α는 **시간 간격 파라미터**로, 학습의 속도에 영향을 준다. 이 갱신 방법은 갱신과정의 변화량이 두 연속

적인 시각의 추정값 차이에 기반하여 계산되기에 **시간차 학습**으로 불린다.



위에서 설명한 방법은 이 문제에 적합한데, 시간 간격 파라미터가 시간이 지남에 따라 적절히 줄어든다는

것은 이 방법을 통해 숫자 표의 확률값이 각각의 상태로부터 승리할 확률의 참값으로 수렴한다는 것을 

의미한다. 이런 상황에서 취해지는 이동(탐험적 이동을 제외하고)은 특정 상대방에 최적화 된 이동이다.

시간 간격 파라미터가 0으로 수렴하지 않더라도, 게임을 수행하는 방법을 느리게 변화시키는 상대방에

대해 게임 참여자가 게임을 잘 수행하고 있음을 의미한다.



이 예제는 강화학습의 몇 가지 핵심 특성을 설명하는데,

첫째로, 강화학습은 주변 환경과 상호작용하면서 학습하는 것이다. 

이 예제에 겨우엔 주변 환경은 게임 상대방일 것이다.

둘째로, 강화학습에는 확실한 목표가 있고, 올바른 행동을 위해 학습자가 선택한 행동의 지연된 효과를 

고려하는 계획 또는 예지가 필요하다는 것이다. 

### 1.6 요약

강화학습은 목표 지향적인 학습과 결정을 이해하고 자동화 하기 위한 계산적 접근 방법이다.

강화학습은 지도나 환경에 대한 완벽한 모델을 필요로 하지 않고 환경과 상호작용하면서

학습한다는 점에서 다른 학습방법과 구별된다.



강화학습은 보상의 합을 최대화 하는 방향으로 학습하며, 

이를 위해 탐욕적 선택과 탐험적 선택을 하며 가치함수를 갱신해 나간다.



가치와 가치함수의 개념은 대부분의 강화학습 방법의 핵심으로, 

효율적인 탐색이 이루어 지기 위해선, 가치함수가 중요하다.
