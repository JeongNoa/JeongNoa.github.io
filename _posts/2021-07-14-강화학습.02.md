---
title: "강화학습_2"
categories: "etc"
changefreq : month
permalink: /Reinforcement/2/
---

강화학습.02
------



이 시리즈는 단단한 강화학습 책을 기반으로 공부한 것을 정리한 것이다.



목차

> [2.1 다중 선택 문제](#2.1 다중 선택 문제)

> [2.2 행동 가치 방법](#2.2 행동 가치 방법)

> [2.3 10중 선택 테스트](#2.3 10중 선택 테스트)

> [2.4 점증적 구현](#2.4 점증적 구현)

> [2.5 비정상 문제의 흔적](#2.5 비정상 문제의 흔적)

> [2.6 긍정적 초깃값](#2.6 긍정적 초깃값)

> [2.7 신뢰 상한 행동 선택](#2.7 신뢰 상한 행동 선택)

> [2.8 경사도 다중 선택 알고리즘](#2.8 경사도 다중 선택 알고리즘)

> [2.9 연관 탐색](#2.9 연관 탐색)

>  [2.10 요약](#2.10 요약)







### 2.1 다중 선택 문제



K개의 서로 다른 레버를 가지고 있는 슬롯머신이 있다고 하자. 이중에서 하나의 레버를 반복적으로 

선택해야 하고, 매 선택 후에는 숫자로 된 보상이 주어진다. 이때 보상을 나타내는 값은 정상 확률 분포

(시간에 따라 변하지 않는 확률 분포)로부터 얻어진다. 선택의 목적은 일정 기간동안 주어지는 보상의

총량에 대한 기댓값을 최대화하는 것이다.



이것이 **다중 선택 문제**의 원형이다. 반복적으로 행동을 선택을 계속하면서 최대의 보상을 주는 레버에

집중하도록 하여 보상을 최대로 만드는 것이 선택의 목적이다.



다중 선택 문제에서 k개의 행동 각각에는 그 행동이 선택되었을 때 기대할 수 있는 평균 보상값이 

할당된다. 이 평균 보상값을 **가치**라고 하자. 시간 t에서 행동은 At, 보상은 Rt라 하자. 임의의 행동 

a의 가치 q*(a)는 행동 a가 선택되었을 때 얻는 보상의 기댓값은 다음과 같이 표현된다.



> q*(a) = E[Rt | At = a]



모든 행동의 가치를 알고 있다면 다중 선택 문제는 쉽게 풀리지만, 행동의 가치를 추정할 수는 

있더라도 확실히 알지 못하는 게 전제이다. 시간 t에서 추정한 행동 a의 가치는 Qt(a)로 표현하는데,

 Qt(a)가 q*(a)와 가까울수록 정확한 추정이다.



행동의 가치를 추정한다면 각 시간마다 추정 가치가 최대인 행동을 선택할 수 있는데, 이를 **탐욕적**

**행동**이라 한다. 이는 행동에 가치에 대해 현재까지 알고 있는 지식을 **활용**하는 것이다. 탐욕적 행동이

아닌 다른 행동을 한다면, 비탐욕적 행동의 추정 가치를 상승시킬 수도 있으므로 **탐험**이다. 



단기적으로 보면 탐욕적 행동을 선택하는 것이 바람직하지만, 장기적으로 보상의 크기를 키우기

위해서는 보면 탐험이 더 옳은 선택일 수 있다. 하지만 탐욕적 행동과 탐험은 동시에 할 수 없기 

때문에 이를 활용과 탐험의 갈등으로 인식된다. 따라서 우리는 탐험과 활용의 적적한 균형을 이룰 

필요가 있다.



### 2.2 행동 가치 방법



행동의 가치를 추정하고 추정값으로부터 행동을 선택하도록 결정하는 방법을 총칭하여 

**행동 가치 방법** 이라 한다. 어떤 행동이 같는 참값이 행동이 선택될 때의 평균 보상이라는 걸

생각하면, 이 참값은 실제로 받은 보상의 산술평균을 계산함을 통해 추정가능하다.

>Qt(a) =(시간 t 전에 행동a를 선택한 보상값의 합)/ (시간 t 이전에 행동a를 선택한 횟수)

이때 분모가 커질수록 Qt(a)는 q*(a)값으로 접근하게 될 것이다. 

이런 행동 가치 주정 방법은 관련 보상값에 대한 표본평균을 추정값으로 하기에 **표본평균** 방법

이라 부른다.



가장 간단한 행동 선택 규칙은 추정 가치가 최대인 행동 중 하나를 선택하는 것이다. 

즉, 위에서 말한 탐욕적 행동 중 하나를 선택하는 것이다. 이러한 **탐욕적** 행동 선택 방법은 다음과 같다.



> At = argmaxQt(a)



이런 탐욕적 행동 선택을 대체할 만한 한 가지 대안은 탐욕적 선택을 수행하면서 가끔씩 말하자면

상대적 빈도수 ε을 작은 값으로 유지하면서 탐욕적 선택 대신 모든 행동을 대상으로 무작위 선택을 

하는 것이다.

이런 근사 탐욕적 행동 선택 규칙을 이용한 방법을 **입실론 탐욕적** 방법이라고 한다.

이 방법의 장점은 단계의 개수가 무한으로 커지면 모든 행동이 선택되는 횟수가 무한이 되어

 Qt(a)가 q*(a)로 수렴한다. 다만 실제 이 방법의 효용성은 미지수다.



### 2.3 10중 선택 테스트



(아래의 결과들은 책에서의 결과들이다)

탐욕적 행동 가치 방법과 입실론 행동 가치 방법을 서로 비교해보기 위해 테스트를 만들어 비교해보자.

열 번의 선택을 하는 다중 선택 문제 2000개를 무작위로 생성한다.  아래 그림과 같이 각각의 다중 선택

무제에 대해 행동가치 q*(a)가 평균이 0이고 분산이 1인 정규분포로부터 선택된다. 이런 테스트를 **10중 선택 테스트**라 하자.

![T]({{ site.url }}{{ site.baseurl }}/assets/images/David_Silver/2/2_1.png){: width="90%" height="90%"}{: .align-center}



아래의 그림은 하나의 탐욕적 방법과 ε = 0.01, 0.1일때의 입실론 탐욕적 방법을 앞서 설명한 대로

10중 선택 테스트에서 비교하고 있다.

![T]({{ site.url }}{{ site.baseurl }}/assets/images/David_Silver/2/2_2.png){: width="90%" height="90%"}{: .align-center}



탐욕적 방법은 시작 직후에는 빠르게 향상되지만, 결국 낮은 수준으로 떨어진다. 탐욕적 방법은 대체로

준최적 행동을 수행하는 상황에 처하기에 장기적으론 다른 방법보다 상당히 낮은 성능을 보인다.

입실론 탐욕적 방법은 계속된 탐험을 통해 최적 행동으 식별할 확률을 증가시켰기 때문에

결국에는 더 좋은 성능을 보여준다.



탐욕적 방법 대신 입실론 탐욕적 방법을 사용할 때의 장점은 문제에 따라 다르다. 보상의 분산이 큰

경우 보상에 잡음이 더 크기에 최적행동을 찾기 위해 더 많은 탐험이 필요할 것이고, 입실론 탐욕적

방법이 훨씬 좋은 방법이 될 것이다. 

반면에 분산이 0이라면 탐욕적 방법이 최상의 성능을 보여줄 것이다.

이처럼 강화학습에서는 탐험과 활용사이의 균형이 필요하다. 



### 2.4 점증적 구현



지금까지 알아본 항동 가치 방법은 모두 관측된 보상의 표본 평균으로 행동의 가치를 추정하는데, 

이런 평균을 계산하는 과정에서 효율적으로 계산하는 방법을 살펴보자.



한번 특정 하나의 행동에만 집중해보자. 이 행동이 i번째 선택된 후 받은 보상을 Ri, n-1번 선택된 후

아 행동의 가치 추정값을 Qn이라 하면 Qn은 다음과 같다.



> Qn =  R1+R2+....+Rn-1 / n - 1



이 공식을 더욱 편리하게 바꿔보면 다음과 같다.(증명은 생략한다)



>Qn+1 = Qn + 1/n(Rn - Qn)



이를 일반적으로 표현하면 다음과 같다.



> 새로운 추정값 ← 이전 추정값 + 시간 간격의 크기[목표값 - 이전 추정값]



[목표값 - 이전 추정값]은 추정 **오차**를 나타낸다. 이 오차는 목표값으로 접근해 나갈 때마다 감소한다.

점증적 계산 방법에서는 시간 간격의 크기가 시간 단계마다 다르다. 시간 간격의 크기는 α, 또는 

αt(a)로 나타낸다.



### 2.5 비정상 문제의 흔적



지금까지의 평균값 방법은 정상 다중 선택 문제에 적합하다. 정상 다중 선택 문제는 보상값의 확률 

분포가 시간이 지간이 지나도 변하지 않는다. 하지만 실직적으로는 비정상적인 강화학습 문제가 

자주 등장한다. 이 경웅에는 최근의 보상일수록 큰 가중치를 주고 오래된 보상일 수록 낮은 가중치를

주는 것이 타당하다. 따라서 Qn을 갱신하기 위한 점증적 갱신 규칙은 다음과 같이 수정된다. 

(중간과정 생략)



> Qn+1 = Qn + α[Rn - Qn]
>
> ​           = (1 - α)^n Q1 + Σ α(1- α)^(n-i) Ri



각 가중치들은 앞으로 등장할 보상의 개수가 증가하면서 (1- α) 의 지수에 따라 기하급수적으로 

감소한다. 따라서 이것을 **기하급수적 최신 가중 평균**이라 부르기도 한다.



시간 간격의 크기를 시간에 따라 변화시키는 것이 편리할 때도 있다. 행동 a를 n번째 선택 후 받은 

보상을 처리하는 데 이용할 시간 간격의 크기를 αn(a)라고 표현하자. 이때 무조건 행동 가치 참값으로 

수렴하기 위한 조건은 다음과 같다.



> Σ αn(a) = inf   ,       Σ αn(a)^2 < inf



첫 번째 조건은 확률적 변동성을 극복할 만큼의 큰 시간을 보장하기 위한 조건, 두 번째 조건은 

결국에는 수렴성을 확신할 만큼 충분히 작아진다는 것을 보장한다.



### 2.6 긍정적 초깃값



지금까지의 방법들은 초기 추정값 Q1(a)에 영향을 받았는데, 이는 초깃값만큼 **편중**되어 있다고 할 수 

있다. 표본평균방법에서는 이 편차는 사라지지만, 고정된 α를 이용하는 방법에서는 편차가 줄어들지만

계속 지속된다. 이 편차가 존재하는 것은 대개는 문제가 되지 않고 오히려 도움이 되기도 한다.



행동 가치의 초깃값을 설정하는 것이 탐험을 촉발기 위한 간단한 방법으로 활용될 수도 있다.

앞서 10중 선택 테스트에서 한 것 같이 행동 가치의 초깃값을 0 대신 5로 설정한다고 해보자. 

q*(a)가 0이 평균이고 1이 분산인 걸 감안하면 5라는 초깃값은 매우 긍정적으로 평가한 값이다.

이는 행동 가치 방법이 탐험하도록 이끌어줄 것이다. 어떤 행동이 초기선택 된다 하더라도 그 

보상값에 실망해서 다른 행동을 선택 할 것이기 때문이다. 따라서 가치 추정값이 수렴하기 전에 

꽤 많은 탐험을 하게 해줄 것이다.  



![T]({{ site.url }}{{ site.baseurl }}/assets/images/David_Silver/2/2_3.jpg){: width="90%" height="90%"}{: .align-center}



위 그림을 보면 Q1(a)를 0, 5로 선택한 각각의 성능을 보여준다. 초기에는 긍정적 초기값 방법이 

탐험을 많이 하기에 나쁜 결과를 보이지만, 시간에 따라 탐험이 줄어들면서 궁극적으론 더 나은

성능을 보인다. 이처럼 탐험을 촉진하는 기법을 **긍정적 초깃값**이라고 부른다. 



이 기법은 정상적 문제에는 꽤 효과적일수 있는 방법이지만 비정상적 문제에는 적합하지 않다. 

탐험에 대한 원동력이 일시적이기 때문이다. 



### 2.7 신뢰 상한 행동 선택



행동 가치 추정의 정밀도에 대해는 항상 불확실성이 있기 때문에 탐험은 필요하다. 입실론 탐욕적

행동 선택은 비탐욕적 행동을 시도하도록 강제하지만, 탐욕적 행동에 가까운 행동 또는 불확실한

행동을 차별하지 않고 행동한다.핮지만 실제로 최적 행동이 될 잠재력이 높은 비탐욕적 행동 중

선택하는 것이 더 나을 것이다. 이렇게 하기 위한 한 가지 방법은 다음과 같다.



> At = argmax[Qt(a) + c**√** ( lnt / Nt(a) )]



여기서 Nt(a)는 t 이전에 행동 a가 선택된 횟수를 나타낸다.



이러한 **신뢰 상한**(UCB) 행동 선택의 개념은 제곱근 항을 통해 행동 a의 가치에 대한 추정값의 불확실성이나 

편차를 고려하는 것이다. 이때 식의 최대값은 행동 a의 진짜 가치로 가능한 값의 상한이 되고, c는 이

상한의 신뢰 수준을 결정한다. 행동 a가 선택될 때마다 불확실성은 아마도 감소할 것이므로 Nt(a)가 

분모에 있으면서 Nt(a)가 늘어나면 불확실성은 감소한다. 반면 a가 아닌 다른 행동이 선택될 때마다

t는 증가하지만 Nt(a)는 그대로이므로 불확실성은 증가한다. 결국 더 작은 가치 추정값을 갖는 

행동이나 이미 자주 선택된 행동은 시간에 따라 선택되는 빈도수가 더 작아질 것이다.





![T]({{ site.url }}{{ site.baseurl }}/assets/images/David_Silver/2/2_4.jpg){: width="90%" height="90%"}{: .align-center}



위 그림은 UCB를 적용한 결과와 입실론 탐욕적 방법과 비교한 결과이다. 이처럼 UCB는 종종 좋은

성능을 내지만 나중에 더 어려운 일반적인 강화학습 문제로 확장하는 데에는 입실론 탐욕적 방법보다

더 어렵다. 



### 2.8 경사도 다중 선택 알고리즘



지금까지는 행동의 가치를 추정하고 그 추정값을 이용해 행동을 선택해왔다. 이번에는 Ht(a)로 

표현되는, 각 행동 a에 대한 수치적 **선호도**를 학습해보자.



선호도가 클수록 그 행동이 더 자주 선택되지만, 선호도는 보상이 아니고 한 행동이 다른 행동에 비해

갖는 상대적 선호도 만이 중요하다. 따라서 모든 행동 선호도에 1000만큼 더한다 해도 영향을 

미치지 않는 다는 것이다. 행동이 선택될 확률은 **soft-max 분포**에 따라 다음과 같이 결정된다.



> Pr{At=a} = e^Ht(a) /  Σ e^Ht(b) = *π*t(a)



*π*t(a)는 시각 t에 행동 a를 선택할 확률을 나타낸다. 처음에는 모든 행동의 선호도가 같다.



이 상황에 확률론적 경사도 증가의 개념을 활용한 신경 학습 알고리즘을 적용해보자.  행동 At를 

선택한 후에 보상 Ri를 받는 모든 단계에서 선호도는 다음과 같이 갱신된다.



> Ht+1(At) = Ht(At) + α(Rt - Rt')(1 - *π*t(At))  그리고
>
> Ht+1(a) = Ht(a) + α(Rt - Rt')*π*t(a)               모든 a /= At에 대해 



α > 0는 시간 간격의 크기를 나타내는 파라미터이고 실수 Rt'는 시각 t까지의 모든 보상의 평균이다.

이는 보상을 비교할 비교대상이 되는데, 보상이 이 값보다 크면 미래에 At를 선택할 확률이 증가하고,

작으면 감소한다. 선택되지 않은 행동에 대한 확률은 그 반대로 움직인다. 



![T]({{ site.url }}{{ site.baseurl }}/assets/images/David_Silver/2/2_5.jpg){: width="90%" height="90%"}{: .align-center}



위 그림은 경사도 다중 선택 알고리즘의 결과를 보여준다. 이 문제에선 0대신 4를 평균으로 하고 

분산은 1인 정규 분포로부터 보상의 참값이 선택된다.  이렇게 보상을 일괄적으로 증가시키는 것은 

알고리즘엔 영향을 주지 않는다. 하지만 보상의 비교대상이 누락되면 성능이 상당히 저하된다.



### 2.9 연관 탐색



지금까지는 비연관적 선택만을 다루었다. 즉, 지금까지의 서로 다른 행동을 환경에 연관시킬 필요가 

없었다. 하지만 여러개의 다중 선택 문제가 있고, 매 단계에서 이들 중 하나가 무작위로 

선택된다고 해보자. 그럼 마치 하나의 비정상적 다중 선택 문제가 있고, 참값이 계속 바뀌는 것처럼

느껴질 것이다. 이제까지 다룬 방법을 사용한다면, 행동 가차의 참값이 천천히 변하지 않으면 잘

작동하지 않을 것이다. 



하지만 만약 하나의 다중 선택 문제가 선택될 때 독특한 단서를 준다고 해보자. 예를 들어 행동의 

가치가 변할 때마다 레버의 색깔이 바뀌는 것이다. 이러면 레버의 색깔과 그때의 해당 문제와

최선의 행동을 연관 시킬수도 있다. 이것이 **연관 탐색**의 한 예이다. 



연관 탐색은 최고의 행동을 **탐색**하면서 동시에 이 행동이 최고가 되는 상황을 이 행동과 **연관**시킨다.

연관 탐색 문제는 다중 선택 문제와 완전한 강화학습 문제에 중간쯤에 있다. 정책에 대한 학습을 

포함시키는 것은 완전한 강화학습과 같지만, 각 행동이 그 순간의 보상에만 영향을 주는 것은 

다중 선택 문제와 유사하다. 행동이 바로 그 순간의 보상뿐 아니라 **다음상황**에도 영향을 준다면,

그것이 완전한 강화학습 문제일 것이다.



### 2.10 요약

이번엔 탐험과 활용사이의 균형을 맞추기 위한 여러 가지 방법들을 알아봤다. 입실론 탐욕적 방법은 

하나의 시간구역을 무작위로선택하고, UCB방법은 그때까지 얻은 표본의 수가 더 적은 행동을 

조금더 선호함으로 탐험이 작동하게 한다. 경사도 다중 선택 알고리즘은 행동의 가치가 아닌 행동의

선호도를 추정하고 soft-max분포를 활용해 더 선호되는 생동을 선택한다.



 